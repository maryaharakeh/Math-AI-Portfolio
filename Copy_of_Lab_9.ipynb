{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hgcQpv0RWRB",
        "outputId": "a1695f46-acc2-4eec-95db-96934031fafb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 28, 28]), torch.Size([60000]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import torch\n",
        "#Imports the PyTorch library, enabling the use of its functions and classes in the code.\n",
        "from torchvision import datasets, transforms\n",
        "#Imports specific modules (datasets and transforms) from the torchvision package. datasets contains pre-built datasets like MNIST, and transforms provides functions for data preprocessing.\n",
        "import numpy as np\n",
        "#Imports the NumPy library and assigns it the alias np. NumPy is commonly used for numerical computations in Python.\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "# Defines a data transformation pipeline using transforms.Compose. In this case, it consists of a single transformation, transforms.ToTensor(), which converts PIL images or numpy arrays into PyTorch tensors.\n",
        "\n",
        "# Download and load the MNIST training data\n",
        "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "#Downloads and loads the MNIST training dataset using the datasets.MNIST class. It specifies the root directory to save the data (./data), sets train=True to load the training set, download=True to download the dataset if it's not already available locally, and applies the previously defined transformation (transform=transform).\n",
        "\n",
        "# Extracting the data and targets as tensors\n",
        "train_images = train_data.data\n",
        "#Extracts the images from the MNIST training dataset and assigns them to the variable train_images. The images are stored as torch tensors.\n",
        "train_labels = train_data.targets\n",
        "#Extracts the corresponding labels (targets) from the MNIST training dataset and assigns them to the variable train_labels. The labels are also stored as torch tensors.\n",
        "train_images.shape, train_labels.shape\n",
        "#Outputs the shapes of the train_images tensor and the train_labels tensor, respectively, providing information about the dimensions of the data.\n",
        "\n",
        "#In summary, this code downloads the MNIST dataset, consisting of handwritten digit images and their corresponding labels, and prepares them for training a machine learning model by converting them into PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt #imports library"
      ],
      "metadata": {
        "id": "Pbf8_IHNRtNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "28*28 #product: The significance of \"28*28\" lies in its association with the MNIST dataset, where each image is represented as a 28x28 grid of pixels. This size, 28x28, defines the dimensions of each image in the dataset. In machine learning tasks using MNIST, this size is crucial for tasks such as data preprocessing, model input shape specification, and understanding the complexity of the dataset."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAdnI0j8T3QK",
        "outputId": "bd983ac4-7bef-492a-8fb0-f4495074fb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "784"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0,:,:] #This line selects the first image from the train_images tensor along the first dimension (index 0), returning all rows and columns of the image."
      ],
      "metadata": {
        "id": "OnzkHg7HSLQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_images[0,:,:] #This line selects the first image from the `train_images` tensor and assigns it to a variable"
      ],
      "metadata": {
        "id": "pIfJV1IJSRUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x) #`plt.imshow(x)` displays the image represented by the numpy array or tensor `x` using Matplotlib."
      ],
      "metadata": {
        "id": "A_R5R-6BSVOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[0] #It accesses the first label in the `train_labels` tensor."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6u78OlNSXAU",
        "outputId": "30b7530e-b66c-4e42-98d4-1018b0a05513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #Returns the shape (dimensions) of tensor \\( x \\)."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HhI_OTdSibN",
        "outputId": "47037a5f-fcb9-4aa9-f841-5f4f18eb3893"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.flatten() #This line flattens the tensor `x`, converting it from a multi-dimensional array to a one-dimensional array."
      ],
      "metadata": {
        "id": "wwWDJuPtUHuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.flatten() #assigns to a variable"
      ],
      "metadata": {
        "id": "c9xWQJoZUJ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #This retrieves the shape of the tensor `x`, providing information about its dimensions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRnyzFjQUqOC",
        "outputId": "017e7cd4-0b1a-46d6-9d8d-d996cff27e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([784])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.reshape(28,28) #This reshapes the tensor `x` into a 28x28 matrix."
      ],
      "metadata": {
        "id": "6P847sd4UrZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #gives dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cX6zb9FSUu8_",
        "outputId": "550313cb-2d53-452d-a564-4f67a717e338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = torch.rand(10,784) #This creates a PyTorch tensor `m` of size 10x784 filled with random numbers sampled from a uniform distribution between 0 and 1."
      ],
      "metadata": {
        "id": "hybDXX2gUzPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m #displays"
      ],
      "metadata": {
        "id": "4jtW-Hg4X8hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.shape #This retrieves the shape of the tensor `m`, indicating it has 10 rows and 784 columns."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88lsibjqXSXM",
        "outputId": "f41bd765-f861-430a-de54-516280987ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.flatten() #This line flattens the tensor `x`, converting it from a multi-dimensional array to a one-dimensional array."
      ],
      "metadata": {
        "id": "cNn7kpMdXemx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = x/255.0 #This line scales the values of tensor `x` to a range between 0 and 1 by dividing each element by 255.0."
      ],
      "metadata": {
        "id": "nec5TuEQX4MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x #displays new tensor"
      ],
      "metadata": {
        "id": "HhsqCX84YoTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.shape,x.shape #gives tensor dimensions"
      ],
      "metadata": {
        "id": "5HgSoF3pXXRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m@x #This performs matrix multiplication between tensors `m` and `x`."
      ],
      "metadata": {
        "id": "c8qJZstIXaqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.matmul(m,x) #This computes the matrix multiplication of tensors `m` and `x`, resulting in tensor `y`."
      ],
      "metadata": {
        "id": "1qdbtf0PX2Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y #displays tensor y"
      ],
      "metadata": {
        "id": "O8QJ0Za1YPbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.max(y) #This returns the maximum value contained within tensor `y`."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRgRV4g1ZjSL",
        "outputId": "4741c5bd-981d-4b77-c7fd-74291fc19a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(57.2089)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(y) #This returns the index of the maximum value in tensor `y`."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCKabh6EZomv",
        "outputId": "bc17406e-c04a-457e-c95e-78fd95148428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_images[0:25,:,:]/255.0 #This line scales and selects the first 25 images from the MNIST training dataset, converting their pixel values to a range between 0 and 1."
      ],
      "metadata": {
        "id": "odpJ6O3gZycR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #This retrieves the shape of the tensor `x`, indicating it contains 25 images with dimensions 28x28 each."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1giRzDM6afmN",
        "outputId": "20c36d62-f5ce-4e85-f86a-9b1ebc25a665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = x.reshape(25,784) #This reshapes the tensor `x` into a 2D tensor with 25 rows and 784 columns, essentially flattening each image into a single row."
      ],
      "metadata": {
        "id": "rACrXIODagdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #gives tensor dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nCNSnOcamYj",
        "outputId": "12b7a92a-3681-4607-b93a-99132e15a376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 784])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x  = x.T #This line transposes the tensor `x`, swapping its dimensions."
      ],
      "metadata": {
        "id": "ufx1iJ_IanQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape #gives tensor dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyTMtZeJasox",
        "outputId": "b19838cb-32f4-4914-aab9-d346250bc833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([784, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = m@x #This line computes the matrix multiplication of tensors `m` and `x`, resulting in tensor `y`."
      ],
      "metadata": {
        "id": "UvoW2g0-at06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape #This retrieves the shape of the tensor `y`, indicating its dimensions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPL59VF-a7Oe",
        "outputId": "4274388a-c77c-47c9-8389-b4b343cf8e0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(y) #This line displays the image represented by tensor `y` using matplotlib's imshow function."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "Y_JaBHfKbBB-",
        "outputId": "76b9e927-4e0c-40d2-9837-1c998d31c1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7dac2b3ab0a0>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hey79wS9bEKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}